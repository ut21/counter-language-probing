{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "from typing import Union\n",
    "from jaxtyping import Int\n",
    "from torch import Tensor\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = {\"[pad]\": 0, \"[start]\": 1, \"[end]\": 2, \"(\": 3, \")\": 4, \"[\": 5, \"]\": 6, \"{\": 7, \"}\": 8}\n",
    "HIDDEN_SIZE = 56\n",
    "HEAD_SIZE = 28\n",
    "NUM_LAYERS = 3\n",
    "NUM_HEADS = 2\n",
    "MAX_LEN = 110\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BracketsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_tuples, tokenizer):\n",
    "        self.tokenizer = SimpleTokenizer(\"()[]{}\")\n",
    "        self.strs = [x[0] for x in data_tuples]\n",
    "        self.isbal = torch.tensor([x[1] for x in data_tuples])\n",
    "        self.toks = self.tokenizer.tokenize(self.strs)\n",
    "        self.open_proportion = torch.tensor([(s.count(\"(\")+s.count(\"[\")+s.count(\"{\")) / len(s) for s in self.strs])\n",
    "        self.starts_open = torch.tensor([(s[0] == \"(\" or s[0] == \"[\" or s== \"{\") for s in self.strs]).bool()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.strs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.strs[idx], self.isbal[idx], self.toks[idx]\n",
    "\n",
    "    def to(self, device):\n",
    "        self.isbal = self.isbal.to(device)\n",
    "        self.toks = self.toks.to(device)\n",
    "        self.open_proportion = self.open_proportion.to(device)\n",
    "        self.starts_open = self.starts_open.to(device)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    START_TOKEN = 1\n",
    "    PAD_TOKEN = 0\n",
    "    END_TOKEN = 2\n",
    "    base_d = {\"[start]\": START_TOKEN, \"[pad]\": PAD_TOKEN, \"[end]\": END_TOKEN}\n",
    "\n",
    "    def __init__(self, alphabet: str):\n",
    "        self.alphabet = alphabet\n",
    "        # the 3 is because there are 3 special tokens (defined just above)\n",
    "        self.t_to_i = {**{c: i + 3 for i, c in enumerate(alphabet)}, **self.base_d}\n",
    "        self.i_to_t = {i: c for c, i in self.t_to_i.items()}\n",
    "\n",
    "    def tokenize(self, strs: list[str], max_len=None) -> Int[Tensor, \"batch seq\"]:\n",
    "        def c_to_int(c: str) -> int:\n",
    "            if c in self.t_to_i:\n",
    "                return self.t_to_i[c]\n",
    "            else:\n",
    "                raise ValueError(c)\n",
    "\n",
    "        if isinstance(strs, str):\n",
    "            strs = [strs]\n",
    "\n",
    "        if max_len is None:\n",
    "            max_len = max((max(len(s) for s in strs), 1))\n",
    "\n",
    "        ints = [\n",
    "            [self.START_TOKEN]\n",
    "            + [c_to_int(c) for c in s]\n",
    "            + [self.END_TOKEN]\n",
    "            + [self.PAD_TOKEN] * (max_len - len(s))\n",
    "            for s in strs\n",
    "        ]\n",
    "        return torch.tensor(ints)\n",
    "\n",
    "    def decode(self, tokens) -> list[str]:\n",
    "        assert tokens.ndim >= 2, \"Need to have a batch dimension\"\n",
    "\n",
    "        def int_to_c(c: int) -> str:\n",
    "            if c < len(self.i_to_t):\n",
    "                return self.i_to_t[c]\n",
    "            else:\n",
    "                raise ValueError(c)\n",
    "\n",
    "        return [\n",
    "            \"\".join(\n",
    "                int_to_c(i.item()) for i in seq[1:] if i != self.PAD_TOKEN and i != self.END_TOKEN\n",
    "            )\n",
    "            for seq in tokens\n",
    "        ]\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"SimpleTokenizer({self.alphabet!r})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    with open(\"./dataset.json\") as f:\n",
    "        data_tuples = json.load(f)\n",
    "    data_tuples = data_tuples\n",
    "    random.shuffle(data_tuples)\n",
    "\n",
    "    train_size = int(0.7 * len(data_tuples))\n",
    "    val_size = int(0.1 * len(data_tuples))\n",
    "    test_size = len(data_tuples) - train_size - val_size\n",
    "\n",
    "    train_data = data_tuples[:train_size]\n",
    "    val_data = data_tuples[train_size:train_size+val_size]\n",
    "    test_data = data_tuples[train_size+val_size:]\n",
    "\n",
    "    tokenizer = SimpleTokenizer(\"()[]{}\")\n",
    "    train_dataset = BracketsDataset(train_data, tokenizer).to(device)\n",
    "    val_dataset = BracketsDataset(val_data, tokenizer).to(device)\n",
    "    test_dataset = BracketsDataset(test_data, tokenizer).to(device)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, head_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = head_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.W_q = nn.Linear(hidden_size, num_heads * head_size)\n",
    "        self.W_k = nn.Linear(hidden_size, num_heads * head_size)\n",
    "        self.W_v = nn.Linear(hidden_size, num_heads * head_size)\n",
    "        self.W_o = nn.Linear(num_heads * head_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, hidden_size = x.size()\n",
    "        \n",
    "        # Project and reshape queries, keys, values\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_size ** 0.5)\n",
    "        \n",
    "        # Mask the key matrix to ignore padded tokens\n",
    "        if mask is not None:\n",
    "            # Create a mask that broadcasts across attention heads\n",
    "            mask = mask.unsqueeze(1).unsqueeze(1)  # Shape: [batch_size, 1, 1, seq_len]\n",
    "            \n",
    "            # Create a key-specific mask by repeating the mask for each head\n",
    "            key_mask = mask.expand(-1, self.num_heads, seq_len, -1)\n",
    "            \n",
    "            # Set attention scores to -inf where the key mask is 0 (padded tokens)\n",
    "            scores = scores.masked_fill(key_mask == 0, float('-inf'))\n",
    "        \n",
    "        # Compute attention weights\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        context = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # Transpose and reshape back to original dimensions\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "        \n",
    "        # Project back to hidden size\n",
    "        return self.W_o(context)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_size, head_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(hidden_size, head_size, num_heads)\n",
    "        self.layernorm1 = nn.LayerNorm(hidden_size)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 4 * hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * hidden_size, hidden_size)\n",
    "        )\n",
    "        self.layernorm2 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.attention(x, mask)\n",
    "        x = self.layernorm1(x + attn_output)\n",
    "        mlp_output = self.mlp(x)\n",
    "        return self.layernorm2(x + mlp_output)\n",
    "\n",
    "class BalancedParenthesesModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, max_len, num_layers, num_heads):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        \n",
    "        # Create positional encodings on the correct device\n",
    "        self.register_buffer('positional_encodings', \n",
    "            torch.zeros(1, max_len, hidden_size), \n",
    "            persistent=False\n",
    "        )\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(hidden_size, hidden_size // num_heads, num_heads)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.layernorm_final = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Global pooling and classification\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)  # Global average pooling\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size // 2, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Ensure positional encodings are on the same device as input\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        # Slice and move positional encodings to input device\n",
    "        positional_encodings = self.positional_encodings[:, :seq_len, :].to(x.device)\n",
    "        \n",
    "        # Embedding with positional encodings\n",
    "        x = self.embedding(x) + positional_encodings\n",
    "        \n",
    "        # Prepare the mask\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            mask = mask.to(x.device)\n",
    "        \n",
    "        # Pass through transformer layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        # Final layer normalization\n",
    "        x = self.layernorm_final(x)\n",
    "        \n",
    "        # Global pooling\n",
    "        x = x.transpose(1, 2)  # Change to (batch, hidden, seq)\n",
    "        x = self.global_pool(x).squeeze(-1)  # Global average pooling\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, val_dataset, num_epochs, batch_size, lr, device):\n",
    "    model.to(device)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 1.0]).to(device))\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch, (_, labels, tokens) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            labels.to(device)\n",
    "            tokens.to(device)\n",
    "            \n",
    "            # Create mask for padding\n",
    "            mask = (tokens != VOCAB[\"[pad]\"]).float().to(device)\n",
    "            \n",
    "            output = model(tokens, mask)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_preds = []\n",
    "        val_true = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _, labels, tokens in val_loader:\n",
    "                labels.to(device)\n",
    "                tokens.to(device)\n",
    "                mask = (tokens != VOCAB[\"[pad]\"]).float().to(device)\n",
    "                output = model(tokens, mask)\n",
    "                loss = criterion(output, labels)\n",
    "                \n",
    "                val_losses.append(loss.item())\n",
    "                val_preds.extend(output.argmax(dim=1).cpu().numpy())\n",
    "                val_true.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_loss = np.mean(val_losses)\n",
    "        val_acc = accuracy_score(val_true, val_preds)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss {total_loss/len(train_loader):.4f}, \"\n",
    "              f\"Val Loss {val_loss:.4f}, Val Accuracy {val_acc:.4f}\")\n",
    "        \n",
    "        # Early stopping and model saving\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_dataset, device):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    incorrect = []\n",
    "    with torch.no_grad():\n",
    "        for batch, (_, labels, tokens) in enumerate(test_loader):\n",
    "            labels.to(device)\n",
    "            tokens.to(device)\n",
    "            mask = (tokens != VOCAB[\"[pad]\"]).unsqueeze(1).unsqueeze(2).to(device)\n",
    "            output = model(tokens)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            incorrect.extend((predicted != labels).nonzero())\n",
    "\n",
    "    print(f\"Accuracy: {100*correct / total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BalancedParenthesesModel(len(VOCAB), HIDDEN_SIZE, MAX_LEN, NUM_LAYERS, NUM_HEADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expand(MPSFloatType{[32, 1, 1, 1, 1, 100]}, size=[-1, 2, 100, -1]): the number of sizes provided (4) must be greater or equal to the number of dimensions in the tensor (6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[40], line 22\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataset, val_dataset, num_epochs, batch_size, lr, device)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Create mask for padding\u001b[39;00m\n\u001b[1;32m     20\u001b[0m mask \u001b[38;5;241m=\u001b[39m (tokens \u001b[38;5;241m!=\u001b[39m VOCAB[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[pad]\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 22\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, labels)\n\u001b[1;32m     25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[39], line 109\u001b[0m, in \u001b[0;36mBalancedParenthesesModel.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Pass through transformer layers\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 109\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Final layer normalization\u001b[39;00m\n\u001b[1;32m    112\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm_final(x)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[39], line 60\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 60\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm1(x \u001b[38;5;241m+\u001b[39m attn_output)\n\u001b[1;32m     62\u001b[0m     mlp_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(x)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[39], line 30\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     27\u001b[0m mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Shape: [batch_size, 1, 1, seq_len]\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Create a key-specific mask by repeating the mask for each head\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m key_mask \u001b[38;5;241m=\u001b[39m \u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Set attention scores to -inf where the key mask is 0 (padded tokens)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m scores \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mmasked_fill(key_mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expand(MPSFloatType{[32, 1, 1, 1, 1, 100]}, size=[-1, 2, 100, -1]): the number of sizes provided (4) must be greater or equal to the number of dimensions in the tensor (6)"
     ]
    }
   ],
   "source": [
    "train_model(model, train_dataset, val_dataset, EPOCHS, BATCH_SIZE, LEARNING_RATE, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 53.75\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
