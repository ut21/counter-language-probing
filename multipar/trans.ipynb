{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParenthesisDataset:\n",
    "    def __init__(self, num_samples: int = 10000):\n",
    "        self.brackets = ['()', '[]', '{}']\n",
    "        self.samples, self.labels = self.generate_dataset(num_samples)\n",
    "    \n",
    "    def generate_balanced_string(self, max_length: int = 20) -> str:\n",
    "        \"\"\"Generate a balanced parenthesis string.\"\"\"\n",
    "        possible_brackets = self.brackets\n",
    "        length = random.randint(2, max_length)\n",
    "        stack = []\n",
    "        result = []\n",
    "        \n",
    "        while len(result) < length:\n",
    "            if not stack or random.random() < 0.5:\n",
    "                # Push an opening bracket\n",
    "                bracket = random.choice(possible_brackets)[0]\n",
    "                result.append(bracket)\n",
    "                stack.append(bracket)\n",
    "            else:\n",
    "                # Pop a closing bracket\n",
    "                opening = stack.pop()\n",
    "                closing = {'(': ')', '[': ']', '{': '}'}[opening]\n",
    "                result.append(closing)\n",
    "        \n",
    "        # Ensure all brackets are closed\n",
    "        while stack:\n",
    "            opening = stack.pop()\n",
    "            closing = {'(': ')', '[': ']', '{': '}'}[opening]\n",
    "            result.append(closing)\n",
    "        \n",
    "        return ''.join(result)\n",
    "    \n",
    "    def generate_unbalanced_string(self, max_length: int = 20) -> str:\n",
    "        \"\"\"Generate an unbalanced parenthesis string.\"\"\"\n",
    "        if random.random() < 0.5:\n",
    "            # Mismatched brackets\n",
    "            base_balanced = self.generate_balanced_string(max_length)\n",
    "            mis_index = random.randint(0, len(base_balanced) - 1)\n",
    "            base_list = list(base_balanced)\n",
    "            base_list[mis_index] = random.choice(['(', ')', '[', ']', '{', '}'])\n",
    "            return ''.join(base_list)\n",
    "        else:\n",
    "            # Unbalanced brackets\n",
    "            base = list(self.generate_balanced_string(max_length))\n",
    "            # Remove some closing or opening brackets\n",
    "            remove_count = random.randint(1, len(base) // 2)\n",
    "            for _ in range(remove_count):\n",
    "                remove_index = random.randint(0, len(base) - 1)\n",
    "                base.pop(remove_index)\n",
    "            return ''.join(base)\n",
    "    \n",
    "    def generate_dataset(self, num_samples: int) -> Tuple[List[str], List[int]]:\n",
    "        \"\"\"Generate balanced and unbalanced dataset.\"\"\"\n",
    "        samples = []\n",
    "        labels = []\n",
    "        \n",
    "        for _ in range(num_samples // 2):\n",
    "            samples.append(self.generate_balanced_string())\n",
    "            labels.append(1)  # Balanced\n",
    "            \n",
    "            samples.append(self.generate_unbalanced_string())\n",
    "            labels.append(0)  # Unbalanced\n",
    "        \n",
    "        return samples, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParenthesisTokenizer:\n",
    "    def __init__(self):\n",
    "        self.special_tokens = {\n",
    "            'PAD': 0,\n",
    "            'START': 1,\n",
    "            'END': 2\n",
    "        }\n",
    "        self.token_to_index = {\n",
    "            '(': 3, ')': 4,\n",
    "            '[': 5, ']': 6,\n",
    "            '{': 7, '}': 8\n",
    "        }\n",
    "        self.index_to_token = {v: k for k, v in self.token_to_index.items()}\n",
    "        self.vocab_size = len(self.special_tokens) + len(self.token_to_index)\n",
    "    \n",
    "    def encode(self, text: str, max_length: int = 50) -> List[int]:\n",
    "        \"\"\"Tokenize a string with START and END tokens and padding.\"\"\"\n",
    "        tokens = [self.special_tokens['START']]\n",
    "        tokens.extend([self.token_to_index[char] for char in text])\n",
    "        tokens.append(self.special_tokens['END'])\n",
    "        \n",
    "        # Pad or truncate\n",
    "        if len(tokens) > max_length:\n",
    "            tokens = tokens[:max_length]\n",
    "        else:\n",
    "            tokens += [self.special_tokens['PAD']] * (max_length - len(tokens))\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def create_mask(self, tokens: List[int]) -> torch.Tensor:\n",
    "        \"\"\"Create an attention mask that ignores padding tokens.\"\"\"\n",
    "        return torch.tensor([1 if token != self.special_tokens['PAD'] else 0 for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerParenthesisClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int = 64, \n",
    "                 num_heads: int = 4, num_layers: int = 2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim, \n",
    "            nhead=num_heads,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer, \n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(1, 0, 2)  # (seq_len, batch, embedding_dim)\n",
    "        \n",
    "        # Apply transformer encoder with attention mask\n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=mask == 0)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = x.mean(dim=0)\n",
    "        \n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, tokenizer, batch_size=32, epochs=10):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Shuffle dataset\n",
    "        indices = list(range(len(dataset.samples)))\n",
    "        random.shuffle(indices)\n",
    "        \n",
    "        for i in range(0, len(indices), batch_size):\n",
    "            batch_indices = indices[i:i+batch_size]\n",
    "            \n",
    "            # Prepare batch\n",
    "            batch_samples = [dataset.samples[j] for j in batch_indices]\n",
    "            batch_labels = [dataset.labels[j] for j in batch_indices]\n",
    "            \n",
    "            # Tokenize and create tensors\n",
    "            batch_tokens = [tokenizer.encode(sample) for sample in batch_samples]\n",
    "            batch_masks = [tokenizer.create_mask(tokens) for tokens in batch_tokens]\n",
    "            \n",
    "            batch_tokens = torch.tensor(batch_tokens)\n",
    "            batch_masks = torch.tensor(batch_masks)\n",
    "            batch_labels = torch.tensor(batch_labels, dtype=torch.float32)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_tokens, batch_masks).squeeze()\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(indices)}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, dataset, tokenizer):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = len(dataset.samples)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sample, label in zip(dataset.samples, dataset.labels):\n",
    "            tokens = torch.tensor(tokenizer.encode(sample)).unsqueeze(0)\n",
    "            mask = tokenizer.create_mask(tokenizer.encode(sample)).unsqueeze(0)\n",
    "            \n",
    "            output = model(tokens, mask).squeeze().item()\n",
    "            prediction = 1 if output > 0.5 else 0\n",
    "            \n",
    "            if prediction == label:\n",
    "                correct += 1\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/utkarsh/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m TransformerParenthesisClassifier(\n\u001b[1;32m     12\u001b[0m     vocab_size\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mvocab_size,\n\u001b[1;32m     13\u001b[0m     embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m     14\u001b[0m     num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     15\u001b[0m     num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Train Model\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Evaluate Model\u001b[39;00m\n\u001b[1;32m     22\u001b[0m evaluated_model \u001b[38;5;241m=\u001b[39m evaluate_model(trained_model, dataset, tokenizer)\n",
      "Cell \u001b[0;32mIn[5], line 24\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataset, tokenizer, batch_size, epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m batch_masks \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mcreate_mask(tokens) \u001b[38;5;28;01mfor\u001b[39;00m tokens \u001b[38;5;129;01min\u001b[39;00m batch_tokens]\n\u001b[1;32m     23\u001b[0m batch_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(batch_tokens)\n\u001b[0;32m---> 24\u001b[0m batch_masks \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m batch_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(batch_labels, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Zero gradients\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Create Dataset\n",
    "dataset = ParenthesisDataset(num_samples=10000)\n",
    "\n",
    "# Create Tokenizer\n",
    "tokenizer = ParenthesisTokenizer()\n",
    "\n",
    "# Create Model\n",
    "model = TransformerParenthesisClassifier(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    embedding_dim=64,\n",
    "    num_heads=4,\n",
    "    num_layers=2\n",
    ")\n",
    "\n",
    "# Train Model\n",
    "trained_model = train_model(model, dataset, tokenizer)\n",
    "\n",
    "# Evaluate Model\n",
    "evaluated_model = evaluate_model(trained_model, dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
