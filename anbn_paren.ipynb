{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "from typing import Union\n",
    "from jaxtyping import Int\n",
    "from torch import Tensor\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = {\"[pad]\": 0, \"[start]\": 1, \"[end]\": 2, \"a\": 3, \"b\": 4}\n",
    "HIDDEN_SIZE = 56\n",
    "HEAD_SIZE = 28\n",
    "NUM_LAYERS = 3\n",
    "NUM_HEADS = 2\n",
    "MAX_LEN = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleTokenizer:\n",
    "    START_TOKEN = 0\n",
    "    PAD_TOKEN = 1\n",
    "    END_TOKEN = 2\n",
    "    base_d = {\"[start]\": START_TOKEN, \"[pad]\": PAD_TOKEN, \"[end]\": END_TOKEN}\n",
    "\n",
    "    def __init__(self, alphabet: str):\n",
    "        self.alphabet = alphabet\n",
    "        # the 3 is because there are 3 special tokens (defined just above)\n",
    "        self.t_to_i = {**{c: i + 3 for i, c in enumerate(alphabet)}, **self.base_d}\n",
    "        self.i_to_t = {i: c for c, i in self.t_to_i.items()}\n",
    "\n",
    "    def tokenize(self, strs: list[str], max_len=None) -> Int[Tensor, \"batch seq\"]:\n",
    "        def c_to_int(c: str) -> int:\n",
    "            if c in self.t_to_i:\n",
    "                return self.t_to_i[c]\n",
    "            else:\n",
    "                raise ValueError(c)\n",
    "\n",
    "        if isinstance(strs, str):\n",
    "            strs = [strs]\n",
    "\n",
    "        if max_len is None:\n",
    "            max_len = max((max(len(s) for s in strs), 1))\n",
    "\n",
    "        ints = [\n",
    "            [self.START_TOKEN]\n",
    "            + [c_to_int(c) for c in s]\n",
    "            + [self.END_TOKEN]\n",
    "            + [self.PAD_TOKEN] * (max_len - len(s))\n",
    "            for s in strs\n",
    "        ]\n",
    "        return torch.tensor(ints)\n",
    "\n",
    "    def decode(self, tokens) -> list[str]:\n",
    "        assert tokens.ndim >= 2, \"Need to have a batch dimension\"\n",
    "\n",
    "        def int_to_c(c: int) -> str:\n",
    "            if c < len(self.i_to_t):\n",
    "                return self.i_to_t[c]\n",
    "            else:\n",
    "                raise ValueError(c)\n",
    "\n",
    "        return [\n",
    "            \"\".join(\n",
    "                int_to_c(i.item()) for i in seq[1:] if i != self.PAD_TOKEN and i != self.END_TOKEN\n",
    "            )\n",
    "            for seq in tokens\n",
    "        ]\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"SimpleTokenizer({self.alphabet!r})\"\n",
    "\n",
    "\n",
    "class BracketsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_tuples, tokenizer):\n",
    "        self.tokenizer = SimpleTokenizer(\"ab\")\n",
    "        self.strs = [x[0] for x in data_tuples]\n",
    "        self.isbal = torch.tensor([x[1] for x in data_tuples])\n",
    "        self.toks = self.tokenizer.tokenize(self.strs)\n",
    "        self.open_proportion = torch.tensor([s.count(\"a\") / len(s) for s in self.strs])\n",
    "        self.starts_open = torch.tensor([s[0] == \"a\" for s in self.strs]).bool()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.strs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.strs[idx], self.isbal[idx], self.toks[idx]\n",
    "\n",
    "    def to(self, device):\n",
    "        self.isbal = self.isbal.to(device)\n",
    "        self.toks = self.toks.to(device)\n",
    "        self.open_proportion = self.open_proportion.to(device)\n",
    "        self.starts_open = self.starts_open.to(device)\n",
    "        return self\n",
    "\n",
    "def load_data():\n",
    "    with open(\"/Users/utkarsh/Documents/neural-toc/naacl_work/anbn_data.json\") as f:\n",
    "        data_tuples = json.load(f)\n",
    "    data_tuples = data_tuples\n",
    "    random.shuffle(data_tuples)\n",
    "\n",
    "    train_size = int(0.8 * len(data_tuples))\n",
    "    val_size = int(0.1 * len(data_tuples))\n",
    "    test_size = len(data_tuples) - train_size - val_size\n",
    "\n",
    "    train_data = data_tuples[:train_size]\n",
    "    val_data = data_tuples[train_size:train_size+val_size]\n",
    "    test_data = data_tuples[train_size+val_size:]\n",
    "\n",
    "    tokenizer = SimpleTokenizer(\"ab\")\n",
    "    train_dataset = BracketsDataset(train_data, tokenizer).to(device)\n",
    "    val_dataset = BracketsDataset(val_data, tokenizer).to(device)\n",
    "    test_dataset = BracketsDataset(test_data, tokenizer).to(device)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, head_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = head_size\n",
    "        self.W_q = nn.Linear(hidden_size, num_heads * head_size)\n",
    "        self.W_k = nn.Linear(hidden_size, num_heads * head_size)\n",
    "        self.W_v = nn.Linear(hidden_size, num_heads * head_size)\n",
    "        self.W_o = nn.Linear(num_heads * head_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, hidden_size = x.size()\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.head_size).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_size ** 0.5)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attention_weights, V).transpose(1, 2).contiguous()\n",
    "        context = context.view(batch_size, seq_len, -1)\n",
    "        return self.W_o(context)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_size, head_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(hidden_size, head_size, num_heads)\n",
    "        self.layernorm1 = nn.LayerNorm(hidden_size)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 4 * hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * hidden_size, hidden_size)\n",
    "        )\n",
    "        self.layernorm2 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.attention(x, mask)\n",
    "        x = self.layernorm1(x + attn_output)\n",
    "        mlp_output = self.mlp(x)\n",
    "        return self.layernorm2(x + mlp_output)\n",
    "\n",
    "# class BalancedParenthesesModel(nn.Module):\n",
    "#     def __init__(self, vocab_size, hidden_size, max_len, num_layers, num_heads):\n",
    "#         super().__init__()\n",
    "#         self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "#         self.positional_encodings = nn.Parameter(torch.zeros(1, max_len, hidden_size))\n",
    "#         self.layers = nn.ModuleList([\n",
    "#             TransformerBlock(hidden_size, HEAD_SIZE, num_heads)\n",
    "#             for _ in range(num_layers)\n",
    "#         ])\n",
    "#         self.layernorm_final = nn.LayerNorm(hidden_size)\n",
    "#         self.unembedding = nn.Linear(hidden_size, 2)\n",
    "\n",
    "#     def forward(self, x, mask=None):\n",
    "#         seq_len = x.size(1)\n",
    "#         x = self.embedding(x) + self.positional_encodings[:, :seq_len, :]\n",
    "#         for layer in self.layers:\n",
    "#             x = layer(x, mask)\n",
    "#         x = self.layernorm_final(x)\n",
    "#         logits = self.unembedding(x[:, 0, :])\n",
    "#         return logits\n",
    "\n",
    "class BalancedParenthesesModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, max_len, num_layers, num_heads):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.positional_encodings = nn.Parameter(torch.zeros(1, max_len, hidden_size))\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(hidden_size, HEAD_SIZE, num_heads)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.layernorm_final = nn.LayerNorm(hidden_size)\n",
    "        self.unembedding = nn.Linear(hidden_size, 2)\n",
    "        \n",
    "        # Store intermediate states during forward pass\n",
    "        self.token_states = None\n",
    "\n",
    "    def forward(self, x, mask=None, return_states=False):\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        # Embedding with positional encodings\n",
    "        x = self.embedding(x) + self.positional_encodings[:, :seq_len, :]\n",
    "        \n",
    "        # Pass through transformer layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        # Final layer normalization\n",
    "        x = self.layernorm_final(x)\n",
    "        \n",
    "        # Compute logits\n",
    "        logits = self.unembedding(x[:, 0, :])\n",
    "        \n",
    "        # Store token states if requested\n",
    "        if return_states:\n",
    "            return logits, x.detach()\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_model(model, train_dataset, val_dataset, num_epochs, batch_size, lr, device):\n",
    "    \"\"\"\n",
    "    Train the Transformer model on the brackets dataset.\n",
    "    \n",
    "    Args:\n",
    "        model (TransformerModel): The Transformer model to train.\n",
    "        train_dataset (BracketsDataset): The training dataset.\n",
    "        val_dataset (BracketsDataset): The validation dataset.\n",
    "        num_epochs (int): The number of training epochs.\n",
    "        batch_size (int): The batch size.\n",
    "        lr (float): The learning rate.\n",
    "        device (torch.device): The device to use for training (CPU or GPU).\n",
    "    \"\"\"\n",
    "    # Set up data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Set up the optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Move the model to the specified device\n",
    "    model.to(device)\n",
    "\n",
    "    # Train the model\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch, (_, labels, tokens) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(tokens)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for _, (_, labels, tokens) in enumerate(val_loader):\n",
    "                mask = (tokens != VOCAB[\"[pad]\"]).unsqueeze(1).unsqueeze(2).to(device)\n",
    "                output = model(tokens, mask)\n",
    "                loss = criterion(output, labels)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        # Print training and validation results\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.6923, Val Loss: 0.6257\n",
      "Epoch 2/50, Train Loss: 0.7290, Val Loss: 0.6119\n",
      "Epoch 3/50, Train Loss: 0.6491, Val Loss: 0.6721\n",
      "Epoch 4/50, Train Loss: 0.6804, Val Loss: 0.6129\n",
      "Epoch 5/50, Train Loss: 0.6605, Val Loss: 0.5671\n",
      "Epoch 6/50, Train Loss: 0.6429, Val Loss: 0.5664\n",
      "Epoch 7/50, Train Loss: 0.6652, Val Loss: 0.5769\n",
      "Epoch 8/50, Train Loss: 0.6457, Val Loss: 0.6018\n",
      "Epoch 9/50, Train Loss: 0.6511, Val Loss: 0.6139\n",
      "Epoch 10/50, Train Loss: 0.6298, Val Loss: 0.5816\n",
      "Epoch 11/50, Train Loss: 0.6628, Val Loss: 0.5636\n",
      "Epoch 12/50, Train Loss: 0.6438, Val Loss: 0.5718\n",
      "Epoch 13/50, Train Loss: 0.6590, Val Loss: 0.5904\n",
      "Epoch 14/50, Train Loss: 0.6516, Val Loss: 0.6095\n",
      "Epoch 15/50, Train Loss: 0.6440, Val Loss: 0.6142\n",
      "Epoch 16/50, Train Loss: 0.6737, Val Loss: 0.5895\n",
      "Epoch 17/50, Train Loss: 0.6666, Val Loss: 0.5976\n",
      "Epoch 18/50, Train Loss: 0.6410, Val Loss: 0.6148\n",
      "Epoch 19/50, Train Loss: 0.6515, Val Loss: 0.5951\n",
      "Epoch 20/50, Train Loss: 0.6631, Val Loss: 0.5847\n",
      "Epoch 21/50, Train Loss: 0.6289, Val Loss: 0.5968\n",
      "Epoch 22/50, Train Loss: 0.6258, Val Loss: 0.5885\n",
      "Epoch 23/50, Train Loss: 0.6360, Val Loss: 0.5751\n",
      "Epoch 24/50, Train Loss: 0.6336, Val Loss: 0.5732\n",
      "Epoch 25/50, Train Loss: 0.6402, Val Loss: 0.5780\n",
      "Epoch 26/50, Train Loss: 0.6680, Val Loss: 0.5904\n",
      "Epoch 27/50, Train Loss: 0.6403, Val Loss: 0.6172\n",
      "Epoch 28/50, Train Loss: 0.6327, Val Loss: 0.6010\n",
      "Epoch 29/50, Train Loss: 0.6173, Val Loss: 0.5765\n",
      "Epoch 30/50, Train Loss: 0.6507, Val Loss: 0.5736\n",
      "Epoch 31/50, Train Loss: 0.6489, Val Loss: 0.5814\n",
      "Epoch 32/50, Train Loss: 0.6388, Val Loss: 0.6041\n",
      "Epoch 33/50, Train Loss: 0.6583, Val Loss: 0.5898\n",
      "Epoch 34/50, Train Loss: 0.6499, Val Loss: 0.5865\n",
      "Epoch 35/50, Train Loss: 0.6469, Val Loss: 0.5940\n",
      "Epoch 36/50, Train Loss: 0.6342, Val Loss: 0.6028\n",
      "Epoch 37/50, Train Loss: 0.6450, Val Loss: 0.5827\n",
      "Epoch 38/50, Train Loss: 0.6531, Val Loss: 0.5831\n",
      "Epoch 39/50, Train Loss: 0.6556, Val Loss: 0.5943\n",
      "Epoch 40/50, Train Loss: 0.6317, Val Loss: 0.6204\n",
      "Epoch 41/50, Train Loss: 0.6422, Val Loss: 0.6111\n",
      "Epoch 42/50, Train Loss: 0.6510, Val Loss: 0.6055\n",
      "Epoch 43/50, Train Loss: 0.6710, Val Loss: 0.6198\n",
      "Epoch 44/50, Train Loss: 0.6463, Val Loss: 0.6367\n",
      "Epoch 45/50, Train Loss: 0.6497, Val Loss: 0.6339\n",
      "Epoch 46/50, Train Loss: 0.6553, Val Loss: 0.6252\n",
      "Epoch 47/50, Train Loss: 0.6386, Val Loss: 0.6150\n",
      "Epoch 48/50, Train Loss: 0.6283, Val Loss: 0.6080\n",
      "Epoch 49/50, Train Loss: 0.6281, Val Loss: 0.6084\n",
      "Epoch 50/50, Train Loss: 0.6361, Val Loss: 0.6184\n",
      "Training complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BalancedParenthesesModel(\n",
       "  (embedding): Embedding(5, 56)\n",
       "  (layers): ModuleList(\n",
       "    (0-2): 3 x TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=56, out_features=56, bias=True)\n",
       "        (W_k): Linear(in_features=56, out_features=56, bias=True)\n",
       "        (W_v): Linear(in_features=56, out_features=56, bias=True)\n",
       "        (W_o): Linear(in_features=56, out_features=56, bias=True)\n",
       "      )\n",
       "      (layernorm1): LayerNorm((56,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=56, out_features=224, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=224, out_features=56, bias=True)\n",
       "      )\n",
       "      (layernorm2): LayerNorm((56,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layernorm_final): LayerNorm((56,), eps=1e-05, elementwise_affine=True)\n",
       "  (unembedding): Linear(in_features=56, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, val_data, test_data = load_data()\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "model = BalancedParenthesesModel(len(VOCAB), HIDDEN_SIZE, MAX_LEN, NUM_LAYERS, NUM_HEADS)\n",
    "train_model(model, train_data, val_data, EPOCHS, BATCH_SIZE, LEARNING_RATE, device=\"mps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = SimpleTokenizer(\"ab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 64.29%\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "correct = 0\n",
    "total = 0\n",
    "incorrect = []\n",
    "with torch.no_grad():\n",
    "    for batch, (_, labels, tokens) in enumerate(test_loader):\n",
    "        output = model(tokens)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if (predicted != labels).any():\n",
    "            incorrect.append((tokeniser.decode(tokens), labels, predicted))\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
