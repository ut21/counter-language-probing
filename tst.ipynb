{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mode': 'train',\n",
       " 'debug': False,\n",
       " 'load_model': False,\n",
       " 'results': True,\n",
       " 'run_name': 'testrun_dyck1_4h',\n",
       " 'display_freq': 35,\n",
       " 'dataset': 'Dyck-1',\n",
       " 'vocab_size': 4,\n",
       " 'histogram': True,\n",
       " 'gpu': 0,\n",
       " 'seed': 1729,\n",
       " 'logging': 1,\n",
       " 'ckpt': 'model',\n",
       " 'emb_size': 64,\n",
       " 'model_type': 'SAN',\n",
       " 'cell_type': 'LSTM',\n",
       " 'hidden_size': 64,\n",
       " 'depth': 1,\n",
       " 'dropout': 0.0,\n",
       " 'max_length': 35,\n",
       " 'bptt': 35,\n",
       " 'use_emb': False,\n",
       " 'init_range': 0.08,\n",
       " 'tied': False,\n",
       " 'generalize': True,\n",
       " 'd_model': 32,\n",
       " 'd_ffn': 64,\n",
       " 'heads': 4,\n",
       " 'pos_encode': False,\n",
       " 'max_period': 10000.0,\n",
       " 'pos_encode_type': 'absolute',\n",
       " 'posffn': True,\n",
       " 'bias': True,\n",
       " 'viz': False,\n",
       " 'freeze_emb': False,\n",
       " 'freeze_q': False,\n",
       " 'freeze_k': False,\n",
       " 'freeze_v': False,\n",
       " 'freeze_f': False,\n",
       " 'zero_k': False,\n",
       " 'lr': 0.005,\n",
       " 'decay_patience': 3,\n",
       " 'decay_rate': 0.1,\n",
       " 'max_grad_norm': -0.25,\n",
       " 'batch_size': 32,\n",
       " 'epochs': 25,\n",
       " 'opt': 'rmsprop',\n",
       " 'lang': 'Dyck',\n",
       " 'lower_window': 2,\n",
       " 'upper_window': 100,\n",
       " 'lower_depth': 0,\n",
       " 'upper_depth': -1,\n",
       " 'val_lower_window': 52,\n",
       " 'val_upper_window': 100,\n",
       " 'training_size': 10000,\n",
       " 'test_size': 500,\n",
       " 'memory_size': 50,\n",
       " 'memory_dim': 5,\n",
       " 'num_par': 2,\n",
       " 'p_val': 0.5,\n",
       " 'q_val': 0.25,\n",
       " 'crl_n': 1,\n",
       " 'generate': False,\n",
       " 'leak': False,\n",
       " 'bins': 4,\n",
       " 'bin1_lower_window': 52,\n",
       " 'bin1_upper_window': 100,\n",
       " 'bin1_lower_depth': 0,\n",
       " 'bin1_upper_depth': -1,\n",
       " 'len_incr': 50,\n",
       " 'depth_incr': 5,\n",
       " 'vary_len': False,\n",
       " 'vary_depth': False,\n",
       " 'log_path': 'logs/testrun_dyck1_4h',\n",
       " 'model_path': 'models/testrun_dyck1_4h',\n",
       " 'board_path': './runs/testrun_dyck1_4h',\n",
       " 'result_path': './out/val_results_Dyck-1.json'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.utils.helper import *\n",
    "import pickle\n",
    "\n",
    "with open('models/testrun_dyck1_4h/vocab.p', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "    \n",
    "with open('/Users/aviralgupta/naacl_work/Transformer-Formal-Languages-2/models/testrun_dyck1_4h/config.p', 'rb') as f:\n",
    "    config = pickle.load(f)\n",
    "    \n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['()((()))', '((())((())))', '((((((())(((((((()))))))))))))', '(())()', '(())', '()', '()()', '((((((()))((()((((((()(())))))))()()))))))', '(((()))((())(()((((())))()))))', '((()(())))']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([42, 10]), tensor([ 8, 12, 30,  6,  4,  2,  4, 42, 30, 10]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.dataloader import *\n",
    "\n",
    "data_path = '/Users/aviralgupta/naacl_work/Transformer-Formal-Languages-2/data/Dyck-1/train_corpus.pk'\n",
    "with open(data_path, 'rb') as f:\n",
    "    train_corpus = pickle.load(f)\n",
    "    \n",
    "train_loader = Sampler(train_corpus, vocab, 10)\n",
    "\n",
    "src, _, wd_lens = train_loader.get_batch(0)\n",
    "src.shape, wd_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 50])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_seq(seq, max_length, voc):\n",
    "    seq += [voc.get_id('T') for i in range(max_length - len(seq))]\n",
    "    return seq\n",
    "\n",
    "def sent_to_idx(voc, sent, max_length=-1):\n",
    "    idx_vec = []\n",
    "    for w in sent:\n",
    "        idx = voc.get_id(w)\n",
    "        idx_vec.append(idx)\n",
    "\n",
    "    idx_vec.append(voc.get_id('T'))\n",
    "    idx_vec = pad_seq(idx_vec, max_length+1, voc)\n",
    "    return idx_vec\n",
    "\n",
    "def sents_to_idx(voc, sents):\n",
    "    max_length = max([len(s) for s in sents])\n",
    "    all_indexes = []\n",
    "    for sent in sents:\n",
    "        all_indexes.append(sent_to_idx(voc, sent, max_length))\n",
    "\n",
    "    all_indexes = torch.tensor(all_indexes, dtype= torch.long)\n",
    "    return all_indexes\n",
    "\n",
    "raw = train_corpus.source\n",
    "data_ids = sents_to_idx(vocab, raw)\n",
    "data = data_ids[:, :-1]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n"
     ]
    }
   ],
   "source": [
    "OPEN = vocab.get_id('(')\n",
    "CLOSE = vocab.get_id(')')\n",
    "\n",
    "print(OPEN, CLOSE)\n",
    "\n",
    "def simulate_stack(tokenised_paren):\n",
    "    stack = []\n",
    "    stack_depths = []\n",
    "    for token in tokenised_paren:\n",
    "        token = token.item()\n",
    "        if token == OPEN:\n",
    "            stack.append(token)\n",
    "        elif token == CLOSE:\n",
    "            stack.pop()\n",
    "        stack_depths.append(len(stack))\n",
    "    return stack_depths\n",
    "\n",
    "def create_dataset(model, paren_tokens, device=\"mps\"):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for tokenised_paren in paren_tokens:\n",
    "            output, internal = model.model(tokenised_paren.to(device).unsqueeze(0).T, get_encoder_reps=True)\n",
    "            internal = internal.transpose(0, 1).squeeze(0)\n",
    "            stack_depths = simulate_stack(tokenised_paren)\n",
    "            for i, states in enumerate(internal):\n",
    "                X.append(states.cpu().numpy())\n",
    "                Y.append(stack_depths[i])\n",
    "\n",
    "    X_tensor = torch.tensor(X)\n",
    "    Y_tensor = torch.tensor(Y)\n",
    "    \n",
    "    return X_tensor, Y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 12:25:55.734259: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 32])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import src\n",
    "import torch\n",
    "from src.model import LanguageModel\n",
    "from src.components.transformers import TransformerModel\n",
    "\n",
    "import torch\n",
    "\n",
    "chkpt_path = '/Users/aviralgupta/naacl_work/Transformer-Formal-Languages-2/models/testrun_dyck1_4h/model_25.pt'\n",
    "model = LanguageModel(config, vocab, 'cpu', None)\n",
    "model.load_state_dict(torch.load(chkpt_path, map_location='cpu')['model_state_dict'])\n",
    "\n",
    "a, b = model.model(torch.randint(0, 3, (1, 10)), get_encoder_reps=True)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class ProbingClassifer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(ProbingClassifer, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 128)\n",
    "        self.fc4 = nn.Linear(128, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "X, Y = create_dataset(model, data[:100], \"cpu\")\n",
    "dataset = TensorDataset(X, Y)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "probing_model = ProbingClassifer(32, 18).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(probing_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 1.7005, Accuracy: 0.5614\n",
      "Epoch [2/30], Loss: 1.3666, Accuracy: 0.5852\n",
      "Epoch [3/30], Loss: 1.1622, Accuracy: 0.6260\n",
      "Epoch [4/30], Loss: 1.0140, Accuracy: 0.6496\n",
      "Epoch [5/30], Loss: 0.9678, Accuracy: 0.6520\n",
      "Epoch [6/30], Loss: 0.9581, Accuracy: 0.6512\n",
      "Epoch [7/30], Loss: 0.9323, Accuracy: 0.6550\n",
      "Epoch [8/30], Loss: 0.9248, Accuracy: 0.6576\n",
      "Epoch [9/30], Loss: 0.9184, Accuracy: 0.6554\n",
      "Epoch [10/30], Loss: 0.9186, Accuracy: 0.6576\n",
      "Epoch [11/30], Loss: 0.9137, Accuracy: 0.6560\n",
      "Epoch [12/30], Loss: 0.9468, Accuracy: 0.6502\n",
      "Epoch [13/30], Loss: 0.9079, Accuracy: 0.6606\n",
      "Epoch [14/30], Loss: 0.9039, Accuracy: 0.6638\n",
      "Epoch [15/30], Loss: 0.9044, Accuracy: 0.6592\n",
      "Epoch [16/30], Loss: 0.9019, Accuracy: 0.6670\n",
      "Epoch [17/30], Loss: 0.9027, Accuracy: 0.6710\n",
      "Epoch [18/30], Loss: 0.9012, Accuracy: 0.6648\n",
      "Epoch [19/30], Loss: 0.8992, Accuracy: 0.6640\n",
      "Epoch [20/30], Loss: 0.9164, Accuracy: 0.6624\n",
      "Epoch [21/30], Loss: 0.9193, Accuracy: 0.6580\n",
      "Epoch [22/30], Loss: 0.8932, Accuracy: 0.6648\n",
      "Epoch [23/30], Loss: 0.8894, Accuracy: 0.6682\n",
      "Epoch [24/30], Loss: 0.8933, Accuracy: 0.6606\n",
      "Epoch [25/30], Loss: 0.9098, Accuracy: 0.6598\n",
      "Epoch [26/30], Loss: 0.9532, Accuracy: 0.6554\n",
      "Epoch [27/30], Loss: 0.8908, Accuracy: 0.6688\n",
      "Epoch [28/30], Loss: 0.8939, Accuracy: 0.6614\n",
      "Epoch [29/30], Loss: 0.8871, Accuracy: 0.6660\n",
      "Epoch [30/30], Loss: 0.8887, Accuracy: 0.6664\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "acc_id = []\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    probing_model.train()\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = probing_model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        #reshape target to match output\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "        \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    acc_id.append(accuracy)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
